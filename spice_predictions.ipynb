{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPICE Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "from Models import *\n",
    "from Prompts import *\n",
    "from Requests import *\n",
    "from Entity_Resolver import *\n",
    "from Database_Query import *\n",
    "from Dataset_stats import *\n",
    "from Import import get_file_paths_pathlib, get_file_paths\n",
    "from Export import append_to_dataframe_and_export_pathlib\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turnID</th>\n",
       "      <th>question_type_id</th>\n",
       "      <th>question_type</th>\n",
       "      <th>description</th>\n",
       "      <th>speaker</th>\n",
       "      <th>entities_in_utterance</th>\n",
       "      <th>relations</th>\n",
       "      <th>type_list</th>\n",
       "      <th>utterance</th>\n",
       "      <th>all_response_entities</th>\n",
       "      <th>sparql_query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test#QA_0#QA_90#0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Simple Question (Direct)</td>\n",
       "      <td>Simple Question</td>\n",
       "      <td>USER</td>\n",
       "      <td>{'Q3035075': 'Dominique Colas'}</td>\n",
       "      <td>{'P69': 'educated at'}</td>\n",
       "      <td>{'Q41176': 'building'}</td>\n",
       "      <td>What is the building where Dominique Colas was...</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test#QA_0#QA_90#0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SYSTEM</td>\n",
       "      <td>{'Q3268957': 'Lycée Thiers'}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>Lycée Thiers</td>\n",
       "      <td>{'Q3268957': 'Lycée Thiers'}</td>\n",
       "      <td>SELECT ?x WHERE { wd:Q3035075 wdt:P69 ?x . ?x ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test#QA_0#QA_90#1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Simple Question (Coreferenced)</td>\n",
       "      <td>Simple Question|Single Entity|Indirect</td>\n",
       "      <td>USER</td>\n",
       "      <td>{'Q3035075': 'Dominique Colas'}</td>\n",
       "      <td>{'P184': 'doctoral advisor'}</td>\n",
       "      <td>{'Q502895': 'common name'}</td>\n",
       "      <td>Who has that one as doctor advisor ?</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test#QA_0#QA_90#1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SYSTEM</td>\n",
       "      <td>{'Q15943337': 'Alexandra Goujon'}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>Alexandra Goujon</td>\n",
       "      <td>{'Q15943337': 'Alexandra Goujon'}</td>\n",
       "      <td>SELECT ?x WHERE { ?x wdt:P184 wd:Q3035075 . ?x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test#QA_0#QA_90#2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verification (Boolean) (All)</td>\n",
       "      <td>Verification|3 entities, 2 direct, 2(direct) a...</td>\n",
       "      <td>USER</td>\n",
       "      <td>{'Q15943337': 'Alexandra Goujon', 'Q80721': 'C...</td>\n",
       "      <td>{'P27': 'country of citizenship'}</td>\n",
       "      <td>{'Q15617994': 'designation for an administrati...</td>\n",
       "      <td>Is that person a citizen of Camerota and Prato...</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              turnID  question_type_id                   question_type  \\\n",
       "0  test#QA_0#QA_90#0               1.0        Simple Question (Direct)   \n",
       "1  test#QA_0#QA_90#0               NaN                             NaN   \n",
       "2  test#QA_0#QA_90#1               2.0  Simple Question (Coreferenced)   \n",
       "3  test#QA_0#QA_90#1               NaN                             NaN   \n",
       "4  test#QA_0#QA_90#2               5.0    Verification (Boolean) (All)   \n",
       "\n",
       "                                         description speaker  \\\n",
       "0                                    Simple Question    USER   \n",
       "1                                                NaN  SYSTEM   \n",
       "2             Simple Question|Single Entity|Indirect    USER   \n",
       "3                                                NaN  SYSTEM   \n",
       "4  Verification|3 entities, 2 direct, 2(direct) a...    USER   \n",
       "\n",
       "                               entities_in_utterance  \\\n",
       "0                    {'Q3035075': 'Dominique Colas'}   \n",
       "1                       {'Q3268957': 'Lycée Thiers'}   \n",
       "2                    {'Q3035075': 'Dominique Colas'}   \n",
       "3                  {'Q15943337': 'Alexandra Goujon'}   \n",
       "4  {'Q15943337': 'Alexandra Goujon', 'Q80721': 'C...   \n",
       "\n",
       "                           relations  \\\n",
       "0             {'P69': 'educated at'}   \n",
       "1                                 {}   \n",
       "2       {'P184': 'doctoral advisor'}   \n",
       "3                                 {}   \n",
       "4  {'P27': 'country of citizenship'}   \n",
       "\n",
       "                                           type_list  \\\n",
       "0                             {'Q41176': 'building'}   \n",
       "1                                                 {}   \n",
       "2                         {'Q502895': 'common name'}   \n",
       "3                                                 {}   \n",
       "4  {'Q15617994': 'designation for an administrati...   \n",
       "\n",
       "                                           utterance  \\\n",
       "0  What is the building where Dominique Colas was...   \n",
       "1                                       Lycée Thiers   \n",
       "2               Who has that one as doctor advisor ?   \n",
       "3                                   Alexandra Goujon   \n",
       "4  Is that person a citizen of Camerota and Prato...   \n",
       "\n",
       "               all_response_entities  \\\n",
       "0                                 {}   \n",
       "1       {'Q3268957': 'Lycée Thiers'}   \n",
       "2                                 {}   \n",
       "3  {'Q15943337': 'Alexandra Goujon'}   \n",
       "4                                 {}   \n",
       "\n",
       "                                        sparql_query  \n",
       "0                                                NaN  \n",
       "1  SELECT ?x WHERE { wd:Q3035075 wdt:P69 ?x . ?x ...  \n",
       "2                                                NaN  \n",
       "3  SELECT ?x WHERE { ?x wdt:P184 wd:Q3035075 . ?x...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder_path = pathlib.PurePath('./SPICE_dataset_pp/test/QA_0')\n",
    "file_name = 'QA_90'\n",
    "spice_df = pd.read_csv(pathlib.PurePath(input_folder_path, f\"{file_name}.csv\"))\n",
    "\n",
    "spice_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call LLM APIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Server (Vicuna/LLaMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_question_to_sparql_local_server(model: ModelType, dataframe: pd.DataFrame, question_index: int, print_prompt_template: bool = False) -> Tuple[str, float]:\n",
    "    prompt_template = get_few_shot_chat_history_prompt(dataframe, question_index)\n",
    "    \n",
    "    if print_prompt_template:\n",
    "        print(f\"Prompt template: {prompt_template}\")\n",
    "\n",
    "    return send_to_local_server_chat(prompt_template, model.value, max_tokens=128)\n",
    "     \n",
    "model = ModelType.VICUNA\n",
    "question_index = 6\n",
    "sparql_query = spice_df.iloc[question_index + 1]['sparql_query']\n",
    "result = convert_question_to_sparql_local_server(model, spice_df, question_index, True)\n",
    "print(f\"\"\"\n",
    "Output: {result[0]}\n",
    "Execution time: {result[1]}\n",
    "\"\"\")\n",
    "print(f\"Expected output: {sparql_query}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template: [{'role': 'system', 'content': \"Generate a SPARQL query that answers the given 'Input question:'. Use 'Entities:', 'Relations:' and 'Types:' specified in the prompt to generate the query. The SPARQL query should be compatible with the Wikidata knowledge graph. Prefixes like 'wdt' and 'wd' have already been defined. No language tag is required. Use '?x' as variable name in the SPARQL query. Remember to provide only a SPARQL query in the response without any notes, comments, or explanations.\"}, {'role': 'user', 'content': \"Input question: Is New York City the place of death of Cirilo Villaverde ?\\nEntities: {'Q727043': 'Cirilo Villaverde', 'Q60': 'New York City'}\\nRelations: {'P20': 'place of death'}\\nTypes: {'Q56061': 'administrative territorial entity'}\"}, {'role': 'assistant', 'content': 'SPARQL query: ASK { wd:Q727043 wdt:P20 wd:Q60 .  }'}, {'role': 'user', 'content': \"Input question: How many works of art express Michael Jordan or pain ?\\nEntities: {'Q41421': 'Michael Jordan', 'Q81938': 'pain'}\\nRelations: {'P180': 'depicts'}\\nTypes: {'Q838948': 'work of art'}\"}, {'role': 'assistant', 'content': 'SPARQL query: SELECT (COUNT(DISTINCT ?x) AS ?count) WHERE { { ?x wdt:P180 wd:Q41421 . ?x wdt:P31 wd:Q838948 .  } UNION { ?x wdt:P180 wd:Q81938 . ?x wdt:P31 wd:Q838948 .  } }'}, {'role': 'user', 'content': \"Conversation history:\\nUSER: Which administrative territory is the native country of Cirilo Villaverde ?\\nSYSTEM: {'Q241': 'Cuba'}\\n\\nInput question: Which is the national anthem of that administrative territory ?\\nEntities: {'Q241': 'Cuba'}\\nRelations: {'P85': 'anthem'}\\nTypes: {'Q484692': 'hymn'}\"}, {'role': 'assistant', 'content': 'SPARQL query: SELECT ?x WHERE { wd:Q241 wdt:P85 ?x . ?x wdt:P31 wd:Q484692 .  }'}, {'role': 'user', 'content': \"Input question: What is the building where Dominique Colas was educated ?\\nEntities: {'Q3035075': 'Dominique Colas'}\\nRelations: {'P69': 'educated at'}\\nTypes: {'Q41176': 'building'}\"}]\n",
      "\n",
      "Output: SPARQL query: SELECT ?x WHERE { wd:Q3035075 wdt:P69 ?x . ?x wdt:P31 wd:Q41176 .  }\n",
      "Execution time: 2.257354974746704\n",
      "\n",
      "Expected output: SELECT ?x WHERE { wd:Q3035075 wdt:P69 ?x . ?x wdt:P31 wd:Q41176 .  }\n"
     ]
    }
   ],
   "source": [
    "def convert_question_to_sparql_openai(model: ModelType, dataframe: pd.DataFrame, question_index: int, print_prompt_template: bool = False) -> Tuple[str, float]:\n",
    "    prompt_template = get_few_shot_chat_history_prompt(dataframe, question_index, True)\n",
    "    \n",
    "    if print_prompt_template:\n",
    "        print(f\"Prompt template: {prompt_template}\")\n",
    "\n",
    "    return send_to_openai_chat(prompt_template, model.value, max_tokens=128)\n",
    "    \n",
    "model = ModelType.GPT3\n",
    "question_index = 0\n",
    "sparql_query = spice_df.iloc[index + 1]['sparql_query']\n",
    "result = convert_question_to_sparql_openai(model, spice_df, question_index, True)\n",
    "print(f\"\"\"\n",
    "Output: {result[0]}\n",
    "Execution time: {result[1]}\n",
    "\"\"\")\n",
    "print(f\"Expected output: {sparql_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Predictions According to Question Category Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the full test dataset\n",
    "distribution_path = pathlib.PurePath(\"../SPICE_dataset/test\")\n",
    "# Path to the subset of the test dataset\n",
    "input_path = pathlib.PurePath('./SPICE_dataset_pp/test/')\n",
    "# Path to the output folder for the predictions\n",
    "output_path = pathlib.PurePath('./Results/csv/Predictions/Test')\n",
    "\n",
    "# Choose a model to use\n",
    "model = ModelType.LORA\n",
    "\n",
    "# Choose a prompt generator function according to the prompt type\n",
    "# Note: use system prompt true for OpenAI and false for local server (for the local server, the system message is added in the FastChat library code: fastchat/conversation.py)\n",
    "prompt_type = 'zero-shot-chat-history' # 'zero-shot-chat-history' or 'few-shot-chat-history'\n",
    "prompt_generator = lambda df, x: get_zero_shot_chat_history_prompt(df, x, False)\n",
    "\n",
    "# Choose the size of the subset of the test set\n",
    "sample_size = 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_folder_file_name(input_file: pathlib.Path) -> Dict[str, str]:\n",
    "    ''' Extracts the last folder and file name from a given path '''\n",
    "    return f\"{input_file.parent.name}\", f\"{input_file.stem}\"\n",
    "\n",
    "# file_path = pathlib.PurePath('./SPICE_dataset_pp/test/QA_0/QA_90.csv')\n",
    "# print(extract_folder_file_name(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_files(path: pathlib.Path, model: ModelType, prompt_type: str):\n",
    "    ''' Returns a list of file paths in the folder (path) for a given model and prompt type '''\n",
    "    file_paths = get_file_paths_pathlib(path)\n",
    "    # Exclude all file paths that do not contain the model name and promtp type\n",
    "    file_paths = [file_path for file_path in file_paths if model.value in file_path.parts[-1] and f\"{prompt_type}.csv\" in file_path.parts[-1]]\n",
    "    return file_paths\n",
    "\n",
    "# test_path = pathlib.PurePath('./Results/csv/Predictions/Test')\n",
    "# test_model = ModelType.LORA\n",
    "# test_prompt_type = 'zero-shot-chat-history'\n",
    "# print(len(get_predicted_files(test_path, test_model, test_prompt_type)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_prediction_been_made(input_file: pathlib.Path, predicted_files: List[pathlib.Path], turnId: str) -> bool:\n",
    "    ''' Returns True if a prediction has already been made for the given turnId, otherwise False '''\n",
    "    input_folder_file_name = extract_folder_file_name(input_file)\n",
    "\n",
    "    for predicted_file in predicted_files:\n",
    "        if input_folder_file_name[0] in predicted_file.parts and input_folder_file_name[1] in predicted_file.parts[-1]:\n",
    "            \n",
    "            # Check if the turnId is contained in the predicted_file\n",
    "            dataframe = pd.read_csv(predicted_file)\n",
    "            # Check if column turnID contains turnId\n",
    "            if turnId in dataframe['turnID'].values:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# test_input_file = pathlib.PurePath('./SPICE_dataset_pp/test/QA_0/QA_90.csv')\n",
    "# test_predicted_files = get_predicted_files(pathlib.PurePath('./Results/csv/Predictions/Test'), ModelType.GPT3, 'zero-shot-chat-history')\n",
    "# test_turnId = 'test#QA_0#QA_90#0'\n",
    "# print(has_prediction_been_made(test_input_file, test_predicted_files, test_turnId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_for_subcategory(subcategory: str, missing_predictions: int, model: ModelType, input_path: pathlib.Path, output_path: pathlib.Path, prompt_generator: Callable[[str], str], prompt_type: str):\n",
    "    ''' Creates missing predictions for a given question subcategory and exports them to the output folder '''\n",
    "    # Change the max tokens if needed\n",
    "    max_tokens = 128\n",
    "    predicted_files = get_predicted_files(output_path, model, prompt_type)\n",
    "    input_files = get_file_paths_pathlib(input_path)\n",
    "\n",
    "    for input_file in input_files:\n",
    "        if missing_predictions <= 0:\n",
    "            break\n",
    "\n",
    "        dataframe = pd.read_csv(input_file)\n",
    "\n",
    "        for conv_index, row in dataframe.iterrows():\n",
    "            question_type = row['question_type']\n",
    "            question_description = row['description']\n",
    "            turnId = row['turnID']\n",
    "\n",
    "            if type(question_type) != str or question_type == '':\n",
    "                continue\n",
    "\n",
    "            if type(question_description) == str and question_description != '':\n",
    "                question_type += f\" [{question_description}]\"\n",
    "\n",
    "            # Check if this conversational turn is of concern\n",
    "            if question_type != subcategory:\n",
    "                continue\n",
    "\n",
    "            # Check if this prediction has already been made\n",
    "            if has_prediction_been_made(input_file, predicted_files, turnId):\n",
    "                continue\n",
    "\n",
    "            # Create the prediction\n",
    "            print(f\"Creating prediction for {turnId}\")\n",
    "            prompt_template = prompt_generator(dataframe, conv_index)\n",
    "            folder_file_name = extract_folder_file_name(input_file)\n",
    "            folder_name = folder_file_name[0]\n",
    "            file_name = folder_file_name[1]\n",
    "            export_path = pathlib.Path(output_path.joinpath(folder_name))\n",
    "            start_time = time.time()\n",
    "\n",
    "            if (model is ModelType.LLAMA or model is ModelType.VICUNA or model is ModelType.LORA):\n",
    "                response = send_to_local_server_chat(prompt_template, model.value, max_tokens=max_tokens)\n",
    "            elif (model is ModelType.GPT3):\n",
    "                response = send_to_openai_chat(prompt_template, model.value, max_tokens=max_tokens)\n",
    "\n",
    "            prediction = response[0]\n",
    "            execution_time = response[1]\n",
    "            append_to_dataframe_and_export_pathlib(dataframe, conv_index, prediction, execution_time, model, prompt_type, export_path, file_name)\n",
    "\n",
    "            print(f\"Finished prediction for {turnId} in {time.time() - start_time}s\")\n",
    "            missing_predictions -= 1\n",
    "\n",
    "            # Export desired response\n",
    "            if (conv_index + 1 < len(dataframe)):\n",
    "                append_to_dataframe_and_export_pathlib(dataframe, conv_index + 1, \"\", 0, model, prompt_type, export_path, file_name)\n",
    "    \n",
    "            if missing_predictions <= 0:\n",
    "                break\n",
    "    \n",
    "    if missing_predictions > 0:\n",
    "        print(f\"Could not find enough samples for {subcategory} ({missing_predictions} missing)\")\n",
    "\n",
    "# create_prediction_for_subcategory('Logical Reasoning (All) [Logical|Difference|Single_Relation]', 1, model, input_path, output_path, prompt_generator, prompt_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Required and Existing Predictions for each Question Sub-Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required samples: {'Logical Reasoning (All) [Logical|Difference|Single_Relation|Incomplete]': 0, 'Logical Reasoning (All) [Logical|Difference|Single_Relation]': 1, 'Quantitative Reasoning (All) [Quantitative|Min/Max|Single entity type]': 2, 'Logical Reasoning (All) [Logical|Intersection|Single_Relation|Incomplete]': 2, 'Comparative Reasoning (Count) (All) [Comparative|Count over More/Less|Single entity type|Indirect]': 2, 'Logical Reasoning (All) [Logical|Difference|Multiple_Relation]': 2, 'Comparative Reasoning (All) [Comparative|More/Less|Single entity type|Indirect]': 2, 'Clarification [Comparative|More/Less|Single entity type|Indirect]': 3, 'Clarification [Comparative|Count over More/Less|Single entity type|Indirect]': 3, 'Comparative Reasoning (Count) (All) [Comparative|Count over More/Less|Mult. entity type|Indirect]': 3, 'Comparative Reasoning (All) [Comparative|More/Less|Mult. entity type|Indirect]': 3, 'Clarification [Comparative|More/Less|Mult. entity type|Indirect]': 3, 'Clarification [Comparative|Count over More/Less|Mult. entity type|Indirect]': 3, 'Simple Question (Ellipsis) [Incomplete|object parent is changed, subject and predicate remain same]': 5, 'Clarification [Quantitative|Count|Logical operators|Indirect]': 6, 'Quantitative Reasoning (Count) (All) [Incomplete count-based ques]': 6, 'Quantitative Reasoning (Count) (All) [Quantitative|Count|Logical operators|Indirect]': 6, 'Comparative Reasoning (All)': 6, 'Comparative Reasoning (Count) (All)': 6, 'Clarification [Quantitative|Count|Single entity type|Indirect]': 7, 'Logical Reasoning (All) [Logical|Union|Single_Relation|Incomplete]': 7, 'Quantitative Reasoning (Count) (All) [Quantitative|Count|Single entity type|Indirect]': 7, 'Comparative Reasoning (Count) (All) [Comparative|Count over More/Less|Single entity type|Incomplete]': 10, 'Comparative Reasoning (All) [Comparative|More/Less|Single entity type|Incomplete]': 10, 'Logical Reasoning (All) [Logical|Intersection|Single_Relation]': 10, 'Quantitative Reasoning (Count) (All) [Quantitative|Count over Atleast/ Atmost/ Approx. the same/Equal|Single entity type]': 11, 'Quantitative Reasoning (All) [Quantitative|Atleast/ Atmost/ Approx. the same/Equal|Single entity type]': 11, 'Quantitative Reasoning (Count) (All)': 12, 'Quantitative Reasoning (All) [Quantitative|Min/Max|Mult. entity type]': 15, 'Verification (Boolean) (All) [Verification|3 entities, 2 direct, 2(direct) are query entities, subject is indirect]': 16, 'Comparative Reasoning (All) [Comparative|More/Less|Mult. entity type|Incomplete]': 16, 'Comparative Reasoning (Count) (All) [Comparative|Count over More/Less|Mult. entity type|Incomplete]': 17, 'Verification (Boolean) (All) [Verification|2 entities, one direct and one indirect, subject is indirect]': 17, 'Comparative Reasoning (Count) (All) [Comparative|Count over More/Less|Single entity type]': 17, 'Comparative Reasoning (All) [Comparative|More/Less|Single entity type]': 18, 'Quantitative Reasoning (Count) (All) [Quantitative|Count over Atleast/ Atmost/ Approx. the same/Equal|Mult. entity type]': 20, 'Quantitative Reasoning (All) [Quantitative|Atleast/ Atmost/ Approx. the same/Equal|Mult. entity type]': 20, 'Quantitative Reasoning (Count) (All) [Quantitative|Count|Logical operators]': 21, 'Logical Reasoning (All) [Logical|Intersection|Multiple_Relation]': 21, 'Quantitative Reasoning (Count) (All) [Quantitative|Count|Mult. entity type]': 24, 'Verification (Boolean) (All) [Verification|3 entities, all direct, 2 are query entities]': 24, 'Comparative Reasoning (All) [Comparative|More/Less|Mult. entity type]': 24, 'Comparative Reasoning (Count) (All) [Comparative|Count over More/Less|Mult. entity type]': 24, 'Quantitative Reasoning (Count) (All) [Quantitative|Count|Single entity type]': 25, 'Verification (Boolean) (All) [Verification|one entity, multiple entities (as object) referred indirectly]': 28, 'Logical Reasoning (All) [Logical|Union|Multiple_Relation]': 29, 'Verification (Boolean) (All) [Verification|2 entities, one direct and one indirect, object is indirect]': 31, 'Verification (Boolean) (All) [Verification|2 entities, both direct]': 32, 'Simple Question (Coreferenced) [Simple Question|Mult. Entity]': 34, 'Simple Question (Direct) [Simple Question|Mult. Entity|Indirect]': 34, 'Clarification [Simple Question|Single Entity|Indirect]': 42, 'Simple Question (Coreferenced)': 42, 'Logical Reasoning (All) [Logical|Union|Single_Relation]': 50, 'Simple Question (Ellipsis) [only subject is changed, parent and predicate remains same]': 52, 'Simple Question (Direct) [Simple Question]': 159, 'Simple Question (Coreferenced) [Simple Question|Single Entity|Indirect]': 232, 'Simple Question (Direct) [Simple Question|Single Entity]': 267}\n",
      "Amount of required samples:  1500\n",
      "Missing samples: {'Logical Reasoning (All) [Logical|Difference|Single_Relation|Incomplete]': 0, 'Logical Reasoning (All) [Logical|Difference|Single_Relation]': 0, 'Quantitative Reasoning (All) [Quantitative|Min/Max|Single entity type]': 0, 'Logical Reasoning (All) [Logical|Intersection|Single_Relation|Incomplete]': 0, 'Comparative Reasoning (Count) (All) [Comparative|Count over More/Less|Single entity type|Indirect]': 0, 'Logical Reasoning (All) [Logical|Difference|Multiple_Relation]': 0, 'Comparative Reasoning (All) [Comparative|More/Less|Single entity type|Indirect]': 0, 'Clarification [Comparative|More/Less|Single entity type|Indirect]': 0, 'Clarification [Comparative|Count over More/Less|Single entity type|Indirect]': 0, 'Comparative Reasoning (Count) (All) [Comparative|Count over More/Less|Mult. entity type|Indirect]': 0, 'Comparative Reasoning (All) [Comparative|More/Less|Mult. entity type|Indirect]': 0, 'Clarification [Comparative|More/Less|Mult. entity type|Indirect]': 0, 'Clarification [Comparative|Count over More/Less|Mult. entity type|Indirect]': 0, 'Simple Question (Ellipsis) [Incomplete|object parent is changed, subject and predicate remain same]': 0, 'Clarification [Quantitative|Count|Logical operators|Indirect]': 0, 'Quantitative Reasoning (Count) (All) [Incomplete count-based ques]': 0, 'Quantitative Reasoning (Count) (All) [Quantitative|Count|Logical operators|Indirect]': 0, 'Comparative Reasoning (All)': 0, 'Comparative Reasoning (Count) (All)': 0, 'Clarification [Quantitative|Count|Single entity type|Indirect]': 0, 'Logical Reasoning (All) [Logical|Union|Single_Relation|Incomplete]': 0, 'Quantitative Reasoning (Count) (All) [Quantitative|Count|Single entity type|Indirect]': 0, 'Comparative Reasoning (Count) (All) [Comparative|Count over More/Less|Single entity type|Incomplete]': 0, 'Comparative Reasoning (All) [Comparative|More/Less|Single entity type|Incomplete]': 0, 'Logical Reasoning (All) [Logical|Intersection|Single_Relation]': 0, 'Quantitative Reasoning (Count) (All) [Quantitative|Count over Atleast/ Atmost/ Approx. the same/Equal|Single entity type]': 0, 'Quantitative Reasoning (All) [Quantitative|Atleast/ Atmost/ Approx. the same/Equal|Single entity type]': 0, 'Quantitative Reasoning (Count) (All)': 0, 'Quantitative Reasoning (All) [Quantitative|Min/Max|Mult. entity type]': 0, 'Verification (Boolean) (All) [Verification|3 entities, 2 direct, 2(direct) are query entities, subject is indirect]': 0, 'Comparative Reasoning (All) [Comparative|More/Less|Mult. entity type|Incomplete]': 0, 'Comparative Reasoning (Count) (All) [Comparative|Count over More/Less|Mult. entity type|Incomplete]': 0, 'Verification (Boolean) (All) [Verification|2 entities, one direct and one indirect, subject is indirect]': 0, 'Comparative Reasoning (Count) (All) [Comparative|Count over More/Less|Single entity type]': 0, 'Comparative Reasoning (All) [Comparative|More/Less|Single entity type]': 0, 'Quantitative Reasoning (Count) (All) [Quantitative|Count over Atleast/ Atmost/ Approx. the same/Equal|Mult. entity type]': 0, 'Quantitative Reasoning (All) [Quantitative|Atleast/ Atmost/ Approx. the same/Equal|Mult. entity type]': 0, 'Quantitative Reasoning (Count) (All) [Quantitative|Count|Logical operators]': 0, 'Logical Reasoning (All) [Logical|Intersection|Multiple_Relation]': 0, 'Quantitative Reasoning (Count) (All) [Quantitative|Count|Mult. entity type]': 0, 'Verification (Boolean) (All) [Verification|3 entities, all direct, 2 are query entities]': 0, 'Comparative Reasoning (All) [Comparative|More/Less|Mult. entity type]': 0, 'Comparative Reasoning (Count) (All) [Comparative|Count over More/Less|Mult. entity type]': 0, 'Quantitative Reasoning (Count) (All) [Quantitative|Count|Single entity type]': 0, 'Verification (Boolean) (All) [Verification|one entity, multiple entities (as object) referred indirectly]': 0, 'Logical Reasoning (All) [Logical|Union|Multiple_Relation]': 0, 'Verification (Boolean) (All) [Verification|2 entities, one direct and one indirect, object is indirect]': 0, 'Verification (Boolean) (All) [Verification|2 entities, both direct]': 0, 'Simple Question (Coreferenced) [Simple Question|Mult. Entity]': 0, 'Simple Question (Direct) [Simple Question|Mult. Entity|Indirect]': 0, 'Clarification [Simple Question|Single Entity|Indirect]': 0, 'Simple Question (Coreferenced)': 0, 'Logical Reasoning (All) [Logical|Union|Single_Relation]': 0, 'Simple Question (Ellipsis) [only subject is changed, parent and predicate remains same]': 0, 'Simple Question (Direct) [Simple Question]': 0, 'Simple Question (Coreferenced) [Simple Question|Single Entity|Indirect]': 0, 'Simple Question (Direct) [Simple Question|Single Entity]': 0}\n",
      "Amount of categories with missing samples:  0\n",
      "Amount of missing samples:  0\n"
     ]
    }
   ],
   "source": [
    "required_samples = count_required_predictions_per_subcategory(sample_size, distribution_path)\n",
    "print(f\"Required samples: {required_samples}\")\n",
    "print(\"Amount of required samples: \", sum(required_samples.values()))\n",
    "\n",
    "missing_samples = count_missing_predictions_per_subcategory(sample_size, distribution_path, output_path, model, prompt_type)\n",
    "print(f\"Missing samples: {missing_samples}\")\n",
    "# print length of all samples that have a value other than 0\n",
    "print(\"Amount of categories with missing samples: \", len([x for x in missing_samples.values() if x != 0]))\n",
    "print(\"Amount of missing samples: \", sum(missing_samples.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create All Missing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions_for_distribution(sample_size: int, distribution_path: str, model: ModelType, input_path: str, output_path: str, prompt_generator: Callable[[str], str], prompt_type: str):\n",
    "    missing_samples = count_missing_predictions_per_subcategory(sample_size, distribution_path, output_path, model, prompt_type)\n",
    "    \n",
    "    for _, (missing_sample, missing_predictions) in enumerate(missing_samples.items()):\n",
    "        print(f\"Creating {missing_predictions} predictions for {missing_sample}\")\n",
    "        \n",
    "        create_prediction_for_subcategory(missing_sample, missing_predictions, model, input_path, output_path, prompt_generator, prompt_type)\n",
    "\n",
    "create_predictions_for_distribution(sample_size, distribution_path, model, input_path, output_path, prompt_generator, prompt_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
